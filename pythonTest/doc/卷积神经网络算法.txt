

（1）对于深度神经网络，中间的隐层的输出必须有一个激活函数。否则多个隐层的作用和没有隐层相同。这个激活函数不一定是sigmoid，常见的有sigmoid、tanh、relu等。
（2）对于二分类问题，输出层是sigmoid函数。这是因为sigmoid函数可以把实数域光滑的映射到[0,1]空间。函数值恰好可以解释为属于正类的概率（概率的取值范围是0~1）。另外，sigmoid函数单调递增，连续可导，导数形式非常简单，是一个比较合适的函数
（3）对于多分类问题，输出层就必须是softmax函数了。softmax函数是sigmoid函数的推广
2.Logistic回归模型
  网络模型介绍：参考http://www.cnblogs.com/nsnow/p/4562308.html
  
  卷积神经网络有两种神器可以降低参数数目，第一种神器叫做局部感知野；那么就启动第二级神器，即权值共享
3.卷积的定义：
卷积是图像处理常用的方法,给定输入图像,在输出图像中每一个像素是输入图像中一个小区域中像素的加权平均,其中权值由一个函数定义,这个函数称为卷积核,
比如说卷积公式：R(u，v)=∑∑G（u-i，v-j）f（i，j） ，其中f为输入，G为卷积核。

